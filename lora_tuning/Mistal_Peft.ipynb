{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "import wandb\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_hf = user_secrets.get_secret(\"HUGGINGFACE_API\")\n",
    "secret_wandb = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "!huggingface-cli login --token $secret_hf\n",
    "\n",
    "wandb.login(key = secret_wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from copy import deepcopy\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import accelerate\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.integrations import WandbCallback\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Auto selects device to put model on.\n",
    ")\n",
    "model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "\n",
    "    # lm_head is often excluded.\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peft Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=modules,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LDJnr/Puffin\", split=\"train\")\n",
    "random_sample = dataset[randrange(len(dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=\"Fine tuning mistral 7B\",  # Project name.\n",
    "    name=\"log_dataset\",          # name of the run within this project.\n",
    "    config={                     # Configuration dictionary.\n",
    "        \"split\": \"train\"\n",
    "    },\n",
    "    group=\"dataset\",             # Group runs. This run belongs in \"dataset\".\n",
    "    tags=[\"dataset\"],            # Tags. More dynamic, low-level grouping.\n",
    "    notes=\"Logging subset of Puffin dataset.\",  # Description about the run.\n",
    "    job_type=\"training\",\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(1000):  # Log 1000 instances.\n",
    "    x = dataset[i]\n",
    "    id_ = x[\"id\"]\n",
    "    conversations = x[\"conversations\"]\n",
    "    for idx, response in enumerate(conversations):\n",
    "        data.append([id_, idx, response[\"from\"], response[\"value\"]])\n",
    "\n",
    "\n",
    "table = wandb.Table(data=data, columns=[\"id\", \"idx\", \"from\", \"value\"])\n",
    "run.log({\"first1000_Puffin\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(sample):\n",
    "    \"\"\"Given a sample dictionary with key \"conversations\", format the conversation into a prompt.\n",
    "\n",
    "\n",
    "    Args:\n",
    "      sample: A sample dictionary from a Hugging Face dataset.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "      sample: sample dictionary with \"text\" key for the formatted prompt.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    INTRO = \"Below is a conversation between a user and you.\"\n",
    "    END = \"Instruction: Write a response appropriate to the conversation.\"\n",
    "\n",
    "\n",
    "    conversations = \"\"\n",
    "    for response in sample[\"conversations\"]:\n",
    "      from_, value = response[\"from\"], response[\"value\"]\n",
    "      conversations += f\"<{from_}>: \" + value + \"\\n\"\n",
    "\n",
    "\n",
    "    sample[\"text\"] = \"\\n\\n\".join([INTRO, conversations, END])\n",
    "\n",
    "\n",
    "    return sample\n",
    "\n",
    "format_prompt(random_sample)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max length: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "\n",
    "# Change the max length depending on hardware constraints.\n",
    "max_length = get_max_length(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
